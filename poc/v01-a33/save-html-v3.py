import time
from datetime import datetime
from pathlib import Path
from urllib.parse import urlencode, urlunparse
from playwright.sync_api import sync_playwright, Error as PlaywrightError
from typing import Dict, Any

# ==============================================================================
# 1. CONFIGURA√á√ïES E CONSTANTES DA APLICA√á√ÉO
# Agrupamento de todos os par√¢metros configur√°veis, URLs e paths no in√≠cio.
# ==============================================================================

# --- Par√¢metros de Busca (Configur√°veis pelo Usu√°rio/Execu√ß√£o) ---
# A string de consulta a ser utilizada na busca do STF.
QUERY_STRING: str = "homoafetiva"
# N√∫mero de resultados por p√°gina.
PAGE_SIZE: int = 30
# Indica se a busca deve ocorrer no inteiro teor ("true" ou "false").
PESQUISA_INTEIRO_TEOR: str = "true"
# Define se o navegador ser√° vis√≠vel (True - modo headed) ou em background (False - modo headless).
HEADED_MODE: bool = False

# --- Configura√ß√µes de Paths e Sa√≠da ---
# Caminho do diret√≥rio de destino para salvar os arquivos HTML extra√≠dos.
OUTPUT_DIR: Path = Path("sd-data/projects/CITO/cito/poc/v01-a33/data/html")

# --- Configura√ß√µes de URL e Par√¢metros Fixos ---
# Componentes de base da URL do STF
URL_SCHEME: str = "https"
URL_NETLOC: str = "jurisprudencia.stf.jus.br"
URL_PATH: str = "/pages/search"

# Par√¢metros de filtro fixos que definem o escopo da busca (n√£o mudam entre execu√ß√µes).
# Nota: "processo_classe_processual_unificada_classe_sigla" √© uma lista para ser
# tratada corretamente durante a codifica√ß√£o da URL.
FIXED_QUERY_PARAMS: Dict[str, Any] = {
    "base": "acordaos",
    "sinonimo": "true",
    "plural": "true",
    "radicais": "false",
    "buscaExata": "true",
    "processo_classe_processual_unificada_classe_sigla": ["ADC", "ADI", "ADO", "ADPF"],
    "page": 1,
    "sort": "_score",
    "sortBy": "desc",
}

# --- Configura√ß√µes do Playwright (Robusto) ---
USER_AGENT: str = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/120.0.0.0 Safari/537.36"
)
VIEWPORT_SIZE: Dict[str, int] = {"width": 1280, "height": 800}
LOCALE: str = "pt-BR"

# ==============================================================================
# 2. FUN√á√ÉO DE CONSTRU√á√ÉO DA URL
# ==============================================================================

def build_target_url() -> str:
    """
    Constr√≥i a URL de busca final, combinando de forma segura os par√¢metros fixos
    e os par√¢metros configur√°veis definidos. Lida com m√∫ltiplos valores por chave
    (como as classes processuais) usando a biblioteca urllib.

    Returns:
        str: A URL completa e formatada para a requisi√ß√£o de busca.
    """
    # Combina os par√¢metros fixos e din√¢micos
    dynamic_params = {
        "pesquisa_inteiro_teor": PESQUISA_INTEIRO_TEOR,
        "pageSize": PAGE_SIZE,
        "queryString": QUERY_STRING
    }
    
    all_params = FIXED_QUERY_PARAMS.copy()
    
    # Extrai e remove as classes do dict principal para tratamento especial na serializa√ß√£o
    classes = all_params.pop("processo_classe_processual_unificada_classe_sigla", [])
    
    # Adiciona os par√¢metros din√¢micos
    all_params.update(dynamic_params)
    
    # Prepara a lista de tuplas (chave, valor) para urlencode.
    query_list = []
    
    # Trata par√¢metros simples
    for key, value in all_params.items():
        query_list.append((key, str(value)))
            
    # Adiciona as classes processuais que requerem repeti√ß√£o da chave na URL
    for class_name in classes:
        query_list.append(("processo_classe_processual_unificada_classe_sigla", class_name))

    # Usa urlencode para formatar a string de consulta
    query_string = urlencode(query_list)
    
    # Monta a URL completa usando urlunparse
    url_tuple = (URL_SCHEME, URL_NETLOC, URL_PATH, "", query_string, "")
    url_alvo = urlunparse(url_tuple)
    
    return url_alvo

# ==============================================================================
# 3. FUN√á√ÉO DE RASPAGEM (SCRAPER) E SALVAMENTO
# ==============================================================================

def scrape_and_save_html(url: str, output_path: Path):
    """
    Inicia o navegador Playwright (Chromium), navega at√© a URL alvo,
    aguarda a renderiza√ß√£o completa do conte√∫do JavaScript e salva o HTML
    da p√°gina em um arquivo com timestamp.

    Args:
        url (str): A URL completa a ser raspada.
        output_path (Path): O diret√≥rio de destino para salvar o HTML.
    """

    print(f"üîπ Iniciando raspagem do STF na URL:\n{url}")
    inicio = time.time()

    # Cria o diret√≥rio de sa√≠da, se n√£o existir
    output_path.mkdir(parents=True, exist_ok=True)

    with sync_playwright() as pw:
        # 3.1 Configura√ß√£o do Navegador
        try:
            # Argumentos para aumentar a robustez em ambientes de servidor
            launch_args = [
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
            ]
            browser = pw.chromium.launch(headless=not HEADED_MODE, args=launch_args)
        except PlaywrightError as e:
            print("‚ùå Falha ao iniciar o navegador Playwright. Verifique a instala√ß√£o do Playwright/depend√™ncias.")
            print(f"Detalhes do erro: {e}")
            return

        # 3.2 Configura√ß√£o do Contexto de Navega√ß√£o (Para simular um usu√°rio real)
        context = browser.new_context(
            user_agent=USER_AGENT,
            viewport=VIEWPORT_SIZE,
            locale=LOCALE,
            extra_http_headers={"accept-language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7"},
        )

        # Scripts para evitar detec√ß√£o de automa√ß√£o (anti-bot)
        context.add_init_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        context.add_init_script("window.chrome = { runtime: {} };")
        context.add_init_script("Object.defineProperty(navigator, 'languages', {get: () => ['pt-BR', 'pt']})")

        page = context.new_page()

        # 3.3 Navega√ß√£o e Espera
        try:
            print("üåê Acessando p√°gina e aguardando rede ficar ociosa...")
            # 'networkidle' espera que a atividade de rede diminua significativamente.
            resp = page.goto(url, wait_until="networkidle")
            if resp:
                print(f"üì∂ Status HTTP da resposta: {resp.status}")
                if resp.status >= 400:
                    print(f"‚ö†Ô∏è Alerta: A requisi√ß√£o retornou o status {resp.status}.")
        except Exception as e:
            print(f"‚ùå Erro durante a navega√ß√£o Playwright: {e}")
            browser.close()
            return
            
        # Atraso adicional para garantir a renderiza√ß√£o de componentes JS din√¢micos
        print("‚è≥ Aguardando 3 segundos adicionais para renderiza√ß√£o JS...")
        time.sleep(3) 

        # 3.4 Captura e Salvamento
        html = page.content()
        browser.close()

        # Nome do arquivo com timestamp para garantir unicidade
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        nome_arquivo = f"stf_html_{timestamp}.html"
        caminho = output_path / nome_arquivo

        # Salva o HTML
        with open(caminho, "w", encoding="utf-8") as f:
            f.write(html)

        duracao = time.time() - inicio
        print(f"‚úÖ HTML salvo com sucesso em: {caminho}")
        print(f"‚è±Ô∏è Tempo total: {duracao:.2f} segundos.")

# ==============================================================================
# 4. EXECU√á√ÉO PRINCIPAL
# Define o ponto de entrada do script.
# ==============================================================================

if __name__ == "__main__":
    # Constr√≥i a URL final usando a fun√ß√£o dedicada
    url_alvo = build_target_url()
    
    # Executa a raspagem e salvamento do HTML
    scrape_and_save_html(url_alvo, OUTPUT_DIR)
